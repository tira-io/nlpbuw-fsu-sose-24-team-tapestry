{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_680/3856197895.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pred_latin][\"language\"] = \"de\" # predictions\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['id', 'language'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Classify all latin-languages\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# text_val_latin_only = df[pred_latin][\"text\"]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# model = open(Path().resolve() / \"model.joblib\")\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# predictions = model.predict(text_val_vec2)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m df[pred_latin][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# predictions\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlanguage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Save the predictions\u001b[39;00m\n\u001b[1;32m     98\u001b[0m df\u001b[38;5;241m.\u001b[39mto_json(\n\u001b[1;32m     99\u001b[0m     Path()\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    100\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['id', 'language'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from joblib import dump\n",
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "tira = Client()\n",
    "text = tira.pd.inputs(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "#text = text.set_index(\"id\")\n",
    "labels = tira.pd.truths(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "\n",
    "text = text.set_index(\"id\")\n",
    "df = text.join(labels.set_index(\"id\"))\n",
    "\n",
    "def get_block(*ranges):\n",
    "    block = []\n",
    "    for r in ranges:\n",
    "        r = r.split('-')\n",
    "        block += list(range(int(r[0], 16), int(r[1], 16) + 1))\n",
    "    return block\n",
    "\n",
    "def comp_freq(text, block):\n",
    "    encoded = np.array([ord(c) for c in text])\n",
    "    return np.sum(np.isin(encoded, block)) / len(encoded)\n",
    "\n",
    "freq_vec = np.vectorize(comp_freq, excluded={1})\n",
    "\n",
    "def is_latin(texts):\n",
    "    latin_block = get_block('0041-024F')\n",
    "    freqs = freq_vec(texts, latin_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_latin = is_latin(df[\"text\"])\n",
    "               \n",
    "# Classify all latin-languages\n",
    "text_val_latin_only = df[pred_latin][\"text\"]\n",
    "\n",
    "remove_punctuation = str.maketrans('', '', r\"-()\\\"#/@;:<>{}-=~|.?,\")\n",
    "\n",
    "def PunctFreeLower(texts):\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        cleaned.append((re.sub(r\"[0-9]+\", \"\", (text.translate(remove_punctuation)))).lower())\n",
    "    return cleaned\n",
    "\n",
    "text_val_cleaned = PunctFreeLower(text_val_latin_only[\"text\"])\n",
    "\n",
    "vec2 = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "text_val_vec2 = vec2.transform(text_val_cleaned)\n",
    "   \n",
    "# Load the model and make predictions\n",
    "model = open(Path().resolve() / \"model.joblib\")\n",
    "predictions = model.predict(text_val_vec2)\n",
    "df_pred = pd.DataFrame (predictions)\n",
    "\n",
    "latin_predict = pd.concat([df[pred_latin], df_pred])\n",
    "#df = df[[\"id\", \"language\"]]\n",
    "\n",
    "# Save the predictions\n",
    "latin_predict.to_json(\n",
    "    Path().resolve() / \"predictions.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from joblib import dump\n",
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "tira = Client()\n",
    "text = tira.pd.inputs(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "#text = text.set_index(\"id\")\n",
    "labels = tira.pd.truths(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "\n",
    "text = text.set_index(\"id\")\n",
    "df = text.join(labels.set_index(\"id\"))\n",
    "df[\"pred_lang\"] = pd.Series([\"\"] * 320000, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split texts in latin and non-latin languages\n",
    "def get_block(*ranges):\n",
    "    block = []\n",
    "    for r in ranges:\n",
    "        r = r.split('-')\n",
    "        block += list(range(int(r[0], 16), int(r[1], 16) + 1))\n",
    "    return block\n",
    "\n",
    "def comp_freq(text, block):\n",
    "    encoded = np.array([ord(c) for c in text])\n",
    "    return np.sum(np.isin(encoded, block)) / len(encoded)\n",
    "\n",
    "freq_vec = np.vectorize(comp_freq, excluded={1})\n",
    "\n",
    "def is_latin(texts):\n",
    "    latin_block = get_block('0041-024F')\n",
    "    freqs = freq_vec(texts, latin_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_latin = is_latin(df[\"text\"])\n",
    "\n",
    "# Classify all non-latin-languages\n",
    "text_val_non_latin = df.loc[~pred_latin, ('text')]\n",
    "\n",
    "def is_cyrillic(texts):\n",
    "    cyrillic_block = get_block('0400-04FF', '0500-052F')\n",
    "    freqs = freq_vec(texts, cyrillic_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_cyrillic = is_cyrillic(text_val_non_latin)\n",
    "\n",
    "# TODO: Train NaiveBayes to distinguish russian from bulgarian\n",
    "cyrillic = ~pred_latin.copy()\n",
    "cyrillic[~pred_latin] = pred_cyrillic    # creating a boolean dataframe of full size, where only cyrillic languages are set to true\n",
    "\n",
    "df.loc[cyrillic, ('pred_lang')] = \"ru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31880, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "cyrillic.shape\n",
    "df.loc[cyrillic].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320000,) (64017, 3)\n",
      "(64017,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (64017) does not match length of index (320000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m lang_remainders \u001b[38;5;241m=\u001b[39m classify_remainders(text_val_remainders)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(lang_remainders\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mremainders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred_lang\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m lang_remainders\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (64017) does not match length of index (320000)"
     ]
    }
   ],
   "source": [
    "non_latin_blocks = {'el': '0370-03FF', 'zh': '4E00-9FFF', 'ko': 'AC00-D7AF', 'ur': '0600-06FF'}\n",
    "\n",
    "def classify_remainders(texts):\n",
    "    langs = np.array(list(non_latin_blocks.keys()))\n",
    "    freqs = np.empty(shape=(texts.shape[0], len(langs)))\n",
    "    for i, lang in enumerate(langs):\n",
    "        block = get_block(non_latin_blocks[lang])\n",
    "        freqs[:, i] = freq_vec(texts, block)\n",
    "    preds = langs[np.argmax(freqs, axis=1)]\n",
    "    return preds\n",
    "\n",
    "remainders = ~pred_latin.copy()\n",
    "remainders[~pred_latin] = ~pred_cyrillic    # creating a boolean dataframe of full size, where only non-latin, non-cyrillic languages are set to true\n",
    "text_val_remainders = df.loc[remainders, ('text')] # all non-latin, non-cyrillic texts\n",
    "lang_remainders = classify_remainders(text_val_remainders)\n",
    "df.loc[remainders, ('pred_lang')] = lang_remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[remainders, ('pred_lang')] = lang_remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>pred_lang</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Der Flughafen Berlin Brandenburg verfügt über ...</td>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Успешное развитие общества, однако, возможно л...</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I øvrigt er kendetegnene for en magnetisk svag...</td>\n",
       "      <td>da</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sowohl über den historischen Simon als auch üb...</td>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emmure е формирана през 2003 г., когато Франки...</td>\n",
       "      <td>bg</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399994</th>\n",
       "      <td>Sociální náklady stejně jako soukromé náklady ...</td>\n",
       "      <td>cs</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>Sljedećeg dana, glumac je malo zakasnio na set...</td>\n",
       "      <td>hr</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...</td>\n",
       "      <td>az</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>Nella serie \"Magico Vento\" figura il personagg...</td>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...</td>\n",
       "      <td>zh</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang pred_lang\n",
       "id                                                                      \n",
       "1       Der Flughafen Berlin Brandenburg verfügt über ...   de          \n",
       "2       Успешное развитие общества, однако, возможно л...   ru        ru\n",
       "3       I øvrigt er kendetegnene for en magnetisk svag...   da          \n",
       "4       Sowohl über den historischen Simon als auch üb...   de          \n",
       "5       Emmure е формирана през 2003 г., когато Франки...   bg        ru\n",
       "...                                                   ...  ...       ...\n",
       "399994  Sociální náklady stejně jako soukromé náklady ...   cs          \n",
       "399995  Sljedećeg dana, glumac je malo zakasnio na set...   hr          \n",
       "399996  İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...   az          \n",
       "399998  Nella serie \"Magico Vento\" figura il personagg...   it          \n",
       "399999  經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...   zh        zh\n",
       "\n",
       "[320000 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load the model and make predictions\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(Path()\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(text_val_vec2)\n\u001b[1;32m     20\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[pred_latin, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_lang\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m=\u001b[39m predictions\n\u001b[1;32m     21\u001b[0m df_ \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[:, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_lang\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'predict'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# Classify all latin-languages\n",
    "text_val_latin_only = df.loc[pred_latin, ('text')]\n",
    "\n",
    "remove_punctuation = str.maketrans('', '', r\"-()\\\"#/@;:<>{}-=~|.?,\")\n",
    "\n",
    "def PunctFreeLower(texts):\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        cleaned.append((re.sub(r\"[0-9]+\", \"\", (text.translate(remove_punctuation)))).lower())\n",
    "    return cleaned\n",
    "\n",
    "text_val_cleaned = PunctFreeLower(text_val_latin_only)\n",
    "\n",
    "vec2 = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "text_val_vec2 = vec2.fit_transform(text_val_cleaned)\n",
    "   \n",
    "# Load the model and make predictions\n",
    "model = open(Path().resolve() / \"model.joblib\")\n",
    "predictions = model.predict(text_val_vec2)\n",
    "df.loc[pred_latin, ('pred_lang')] = predictions\n",
    "df_ = df.loc[:, ('pred_lang')]\n",
    "\n",
    "# Save the predictions\n",
    "df_.to_json(\n",
    "    Path().resolve() / \"predictions.jsonl\", orient=\"index\", index=True\n",
    ")\n",
    "# df_.to_json(\n",
    "#     Path().resolve() / \"predictions.jsonl\", orient=\"records\", lines=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='/workspaces/nlpbuw-fsu-sose-24-team-tapestry/language-identification-bayes/model.joblib' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = open(Path().resolve() / \"model.joblib\")\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
