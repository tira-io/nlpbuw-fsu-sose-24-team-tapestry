{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_680/3856197895.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pred_latin][\"language\"] = \"de\" # predictions\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['id', 'language'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Classify all latin-languages\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# text_val_latin_only = df[pred_latin][\"text\"]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# model = open(Path().resolve() / \"model.joblib\")\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# predictions = model.predict(text_val_vec2)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m df[pred_latin][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# predictions\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlanguage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Save the predictions\u001b[39;00m\n\u001b[1;32m     98\u001b[0m df\u001b[38;5;241m.\u001b[39mto_json(\n\u001b[1;32m     99\u001b[0m     Path()\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    100\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['id', 'language'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from joblib import dump\n",
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "tira = Client()\n",
    "text = tira.pd.inputs(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "#text = text.set_index(\"id\")\n",
    "labels = tira.pd.truths(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "\n",
    "text = text.set_index(\"id\")\n",
    "df = text.join(labels.set_index(\"id\")\n",
    "               \n",
    "# Classify all latin-languages\n",
    "text_val_latin_only = df[pred_latin][\"text\"]\n",
    "\n",
    "remove_punctuation = str.maketrans('', '', r\"-()\\\"#/@;:<>{}-=~|.?,\")\n",
    "\n",
    "def PunctFreeLower(texts):\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        cleaned.append((re.sub(r\"[0-9]+\", \"\", (text.translate(remove_punctuation)))).lower())\n",
    "    return cleaned\n",
    "\n",
    "text_val_cleaned = PunctFreeLower(text_val_latin_only[\"text\"])\n",
    "\n",
    "vec2 = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "text_val_vec2 = vec2.fit_transform(text_val_cleaned)\n",
    "   \n",
    "# Load the model and make predictions\n",
    "model = open(Path().resolve() / \"model.joblib\")\n",
    "predictions = model.predict(text_val_vec2)\n",
    "\n",
    "df = df[[\"id\", \"language\"]]\n",
    "\n",
    "# Save the predictions\n",
    "df.to_json(\n",
    "    Path().resolve() / \"predictions.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from joblib import dump\n",
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text lang language\n",
      "id                                                                     \n",
      "1       Der Flughafen Berlin Brandenburg verfügt über ...   de         \n",
      "2       Успешное развитие общества, однако, возможно л...   ru         \n",
      "3       I øvrigt er kendetegnene for en magnetisk svag...   da         \n",
      "4       Sowohl über den historischen Simon als auch üb...   de         \n",
      "5       Emmure е формирана през 2003 г., когато Франки...   bg         \n",
      "...                                                   ...  ...      ...\n",
      "399994  Sociální náklady stejně jako soukromé náklady ...   cs         \n",
      "399995  Sljedećeg dana, glumac je malo zakasnio na set...   hr         \n",
      "399996  İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...   az         \n",
      "399998  Nella serie \"Magico Vento\" figura il personagg...   it         \n",
      "399999  經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...   zh         \n",
      "\n",
      "[320000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "tira = Client()\n",
    "text = tira.pd.inputs(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "#text = text.set_index(\"id\")\n",
    "labels = tira.pd.truths(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "\n",
    "text = text.set_index(\"id\")\n",
    "df = text.join(labels.set_index(\"id\"))\n",
    "df[\"language\"] = pd.Series([\"\"] * 320000, index=df.index)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split texts in latin and non-latin languages\n",
    "def get_block(*ranges):\n",
    "    block = []\n",
    "    for r in ranges:\n",
    "        r = r.split('-')\n",
    "        block += list(range(int(r[0], 16), int(r[1], 16) + 1))\n",
    "    return block\n",
    "\n",
    "def comp_freq(text, block):\n",
    "    encoded = np.array([ord(c) for c in text])\n",
    "    return np.sum(np.isin(encoded, block)) / len(encoded)\n",
    "\n",
    "freq_vec = np.vectorize(comp_freq, excluded={1})\n",
    "\n",
    "def is_latin(texts):\n",
    "    latin_block = get_block('0041-024F')\n",
    "    freqs = freq_vec(texts, latin_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_latin = is_latin(df[\"text\"])\n",
    "\n",
    "# Classify all non-latin-languages\n",
    "text_val_non_latin = df[~pred_latin][\"text\"]\n",
    "\n",
    "def is_cyrillic(texts):\n",
    "    cyrillic_block = get_block('0400-04FF', '0500-052F')\n",
    "    freqs = freq_vec(texts, cyrillic_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_cyrillic = is_cyrillic(text_val_non_latin)\n",
    "\n",
    "# TODO: Train NaiveBayes to distinguish russian from bulgarian\n",
    "df[~pred_latin][pred_cyrillic][\"language\"] = \"ru\"\n",
    "\n",
    "non_latin_blocks = {'el': '0370-03FF', 'zh': '4E00-9FFF', 'ko': 'AC00-D7AF', 'ur': '0600-06FF'}\n",
    "\n",
    "def classify_remainders(texts):\n",
    "    langs = np.array(list(non_latin_blocks.keys()))\n",
    "    freqs = np.empty(shape=(texts.shape[0], len(langs)))\n",
    "    for i, lang in enumerate(langs):\n",
    "        block = get_block(non_latin_blocks[lang])\n",
    "        freqs[:, i] = freq_vec(texts, block)\n",
    "    preds = langs[np.argmax(freqs, axis=1)]\n",
    "    return preds\n",
    "\n",
    "text_val_remainders = df[~pred_latin][~pred_cyrillic][\"text\"] # all non-latin, non-cyrillic texts\n",
    "lang_remainders = classify_remainders(text_val_remainders)\n",
    "df[~pred_latin][~pred_cyrillic][\"language\"] = lang_remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Der Flughafen Berlin Brandenburg verfügt über ...</td>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Успешное развитие общества, однако, возможно л...</td>\n",
       "      <td>ru</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I øvrigt er kendetegnene for en magnetisk svag...</td>\n",
       "      <td>da</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sowohl über den historischen Simon als auch üb...</td>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emmure е формирана през 2003 г., когато Франки...</td>\n",
       "      <td>bg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399994</th>\n",
       "      <td>Sociální náklady stejně jako soukromé náklady ...</td>\n",
       "      <td>cs</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>Sljedećeg dana, glumac je malo zakasnio na set...</td>\n",
       "      <td>hr</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...</td>\n",
       "      <td>az</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>Nella serie \"Magico Vento\" figura il personagg...</td>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...</td>\n",
       "      <td>zh</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang language\n",
       "id                                                                     \n",
       "1       Der Flughafen Berlin Brandenburg verfügt über ...   de         \n",
       "2       Успешное развитие общества, однако, возможно л...   ru         \n",
       "3       I øvrigt er kendetegnene for en magnetisk svag...   da         \n",
       "4       Sowohl über den historischen Simon als auch üb...   de         \n",
       "5       Emmure е формирана през 2003 г., когато Франки...   bg         \n",
       "...                                                   ...  ...      ...\n",
       "399994  Sociální náklady stejně jako soukromé náklady ...   cs         \n",
       "399995  Sljedećeg dana, glumac je malo zakasnio na set...   hr         \n",
       "399996  İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...   az         \n",
       "399998  Nella serie \"Magico Vento\" figura il personagg...   it         \n",
       "399999  經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...   zh         \n",
       "\n",
       "[320000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text lang language\n",
      "id                                                                     \n",
      "1       Der Flughafen Berlin Brandenburg verfügt über ...   de         \n",
      "2       Успешное развитие общества, однако, возможно л...   ru         \n",
      "3       I øvrigt er kendetegnene for en magnetisk svag...   da         \n",
      "4       Sowohl über den historischen Simon als auch üb...   de         \n",
      "5       Emmure е формирана през 2003 г., когато Франки...   bg         \n",
      "...                                                   ...  ...      ...\n",
      "399994  Sociální náklady stejně jako soukromé náklady ...   cs         \n",
      "399995  Sljedećeg dana, glumac je malo zakasnio na set...   hr         \n",
      "399996  İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...   az         \n",
      "399998  Nella serie \"Magico Vento\" figura il personagg...   it         \n",
      "399999  經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...   zh         \n",
      "\n",
      "[320000 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_680/389421962.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pred_latin][\"language\"] = \"de\" # predictions\n"
     ]
    }
   ],
   "source": [
    "# Classify all latin-languages\n",
    "# text_val_latin_only = df[pred_latin][\"text\"]\n",
    "# \n",
    "# remove_punctuation = str.maketrans('', '', r\"-()\\\"#/@;:<>{}-=~|.?,\")\n",
    "# \n",
    "# def PunctFreeLower(texts):\n",
    "#     cleaned = []\n",
    "#     for text in texts:\n",
    "#         cleaned.append((re.sub(r\"[0-9]+\", \"\", (text.translate(remove_punctuation)))).lower())\n",
    "#     return cleaned\n",
    "# \n",
    "# text_val_cleaned = PunctFreeLower(text_val_latin_only[\"text\"])\n",
    "# \n",
    "# vec2 = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "# text_val_vec2 = vec2.fit_transform(text_val_cleaned)\n",
    "#    \n",
    "# # Load the model and make predictions\n",
    "# model = open(Path().resolve() / \"model.joblib\")\n",
    "# predictions = model.predict(text_val_vec2)\n",
    "df[pred_latin][\"language\"] = \"de\" # predictions\n",
    "print(df)\n",
    "df_ = df[\"language\"]\n",
    "\n",
    "# Save the predictions\n",
    "df_.to_json(\n",
    "    Path().resolve() / \"predictions.jsonl\", orient=\"index\", index=True\n",
    ")\n",
    "# df_.to_json(\n",
    "#     Path().resolve() / \"predictions.jsonl\", orient=\"records\", lines=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
