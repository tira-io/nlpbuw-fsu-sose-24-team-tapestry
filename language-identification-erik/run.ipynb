{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m         cleaned\u001b[38;5;241m.\u001b[39mappend((re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[0-9]+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, (text\u001b[38;5;241m.\u001b[39mtranslate(remove_punctuation))))\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned\n\u001b[0;32m---> 57\u001b[0m text_val_cleaned \u001b[38;5;241m=\u001b[39m PunctFreeLower(\u001b[43mtext_val_latin_only\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     59\u001b[0m vec2 \u001b[38;5;241m=\u001b[39m CountVectorizer(analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     60\u001b[0m text_val_vec2 \u001b[38;5;241m=\u001b[39m vec2\u001b[38;5;241m.\u001b[39mtransform(text_val_cleaned)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from joblib import dump\n",
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "tira = Client()\n",
    "text = tira.pd.inputs(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "#text = text.set_index(\"id\")\n",
    "labels = tira.pd.truths(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "\n",
    "text = text.set_index(\"id\")\n",
    "df = text.join(labels.set_index(\"id\"))\n",
    "\n",
    "def get_block(*ranges):\n",
    "    block = []\n",
    "    for r in ranges:\n",
    "        r = r.split('-')\n",
    "        block += list(range(int(r[0], 16), int(r[1], 16) + 1))\n",
    "    return block\n",
    "\n",
    "def comp_freq(text, block):\n",
    "    encoded = np.array([ord(c) for c in text])\n",
    "    return np.sum(np.isin(encoded, block)) / len(encoded)\n",
    "\n",
    "freq_vec = np.vectorize(comp_freq, excluded={1})\n",
    "\n",
    "def is_latin(texts):\n",
    "    latin_block = get_block('0041-024F')\n",
    "    freqs = freq_vec(texts, latin_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_latin = is_latin(df[\"text\"])\n",
    "               \n",
    "# Classify all latin-languages\n",
    "text_val_latin_only = df[pred_latin][\"text\"]\n",
    "\n",
    "remove_punctuation = str.maketrans('', '', r\"-()\\\"#/@;:<>{}-=~|.?,\")\n",
    "\n",
    "def PunctFreeLower(texts):\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        cleaned.append((re.sub(r\"[0-9]+\", \"\", (text.translate(remove_punctuation)))).lower())\n",
    "    return cleaned\n",
    "\n",
    "text_val_cleaned = PunctFreeLower(text_val_latin_only[\"text\"])\n",
    "\n",
    "vec2 = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "text_val_vec2 = vec2.transform(text_val_cleaned)\n",
    "   \n",
    "# Load the model and make predictions\n",
    "model = open(Path().resolve() / \"model.joblib\")\n",
    "predictions = model.predict(text_val_vec2)\n",
    "df_pred = pd.DataFrame (predictions)\n",
    "\n",
    "latin_predict = pd.concat([df[pred_latin], df_pred])\n",
    "#df = df[[\"id\", \"language\"]]\n",
    "\n",
    "# Save the predictions\n",
    "latin_predict.to_json(\n",
    "    Path().resolve() / \"predictions.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from joblib import dump\n",
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text lang language\n",
      "id                                                                     \n",
      "1       Der Flughafen Berlin Brandenburg verfügt über ...   de         \n",
      "2       Успешное развитие общества, однако, возможно л...   ru         \n",
      "3       I øvrigt er kendetegnene for en magnetisk svag...   da         \n",
      "4       Sowohl über den historischen Simon als auch üb...   de         \n",
      "5       Emmure е формирана през 2003 г., когато Франки...   bg         \n",
      "...                                                   ...  ...      ...\n",
      "399994  Sociální náklady stejně jako soukromé náklady ...   cs         \n",
      "399995  Sljedećeg dana, glumac je malo zakasnio na set...   hr         \n",
      "399996  İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...   az         \n",
      "399998  Nella serie \"Magico Vento\" figura il personagg...   it         \n",
      "399999  經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...   zh         \n",
      "\n",
      "[320000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "tira = Client()\n",
    "text = tira.pd.inputs(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "#text = text.set_index(\"id\")\n",
    "labels = tira.pd.truths(\n",
    "    \"nlpbuw-fsu-sose-24\", \"language-identification-train-20240429-training\"\n",
    ")\n",
    "\n",
    "text = text.set_index(\"id\")\n",
    "df = text.join(labels.set_index(\"id\"))\n",
    "df[\"language\"] = pd.Series([\"\"] * 320000, index=df.index)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split texts in latin and non-latin languages\n",
    "def get_block(*ranges):\n",
    "    block = []\n",
    "    for r in ranges:\n",
    "        r = r.split('-')\n",
    "        block += list(range(int(r[0], 16), int(r[1], 16) + 1))\n",
    "    return block\n",
    "\n",
    "def comp_freq(text, block):\n",
    "    encoded = np.array([ord(c) for c in text])\n",
    "    return np.sum(np.isin(encoded, block)) / len(encoded)\n",
    "\n",
    "freq_vec = np.vectorize(comp_freq, excluded={1})\n",
    "\n",
    "def is_latin(texts):\n",
    "    latin_block = get_block('0041-024F')\n",
    "    freqs = freq_vec(texts, latin_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_latin = is_latin(df[\"text\"])\n",
    "\n",
    "# Classify all non-latin-languages\n",
    "text_val_non_latin = df[~pred_latin][\"text\"]\n",
    "\n",
    "def is_cyrillic(texts):\n",
    "    cyrillic_block = get_block('0400-04FF', '0500-052F')\n",
    "    freqs = freq_vec(texts, cyrillic_block)\n",
    "    return freqs > 0.5\n",
    "\n",
    "pred_cyrillic = is_cyrillic(text_val_non_latin)\n",
    "\n",
    "# TODO: Train NaiveBayes to distinguish russian from bulgarian\n",
    "df[~pred_latin][pred_cyrillic][\"language\"] = \"ru\"\n",
    "\n",
    "non_latin_blocks = {'el': '0370-03FF', 'zh': '4E00-9FFF', 'ko': 'AC00-D7AF', 'ur': '0600-06FF'}\n",
    "\n",
    "def classify_remainders(texts):\n",
    "    langs = np.array(list(non_latin_blocks.keys()))\n",
    "    freqs = np.empty(shape=(texts.shape[0], len(langs)))\n",
    "    for i, lang in enumerate(langs):\n",
    "        block = get_block(non_latin_blocks[lang])\n",
    "        freqs[:, i] = freq_vec(texts, block)\n",
    "    preds = langs[np.argmax(freqs, axis=1)]\n",
    "    return preds\n",
    "\n",
    "text_val_remainders = df[~pred_latin][~pred_cyrillic][\"text\"] # all non-latin, non-cyrillic texts\n",
    "lang_remainders = classify_remainders(text_val_remainders)\n",
    "df[~pred_latin][~pred_cyrillic][\"language\"] = lang_remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Der Flughafen Berlin Brandenburg verfügt über ...</td>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Успешное развитие общества, однако, возможно л...</td>\n",
       "      <td>ru</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I øvrigt er kendetegnene for en magnetisk svag...</td>\n",
       "      <td>da</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sowohl über den historischen Simon als auch üb...</td>\n",
       "      <td>de</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emmure е формирана през 2003 г., когато Франки...</td>\n",
       "      <td>bg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399994</th>\n",
       "      <td>Sociální náklady stejně jako soukromé náklady ...</td>\n",
       "      <td>cs</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>Sljedećeg dana, glumac je malo zakasnio na set...</td>\n",
       "      <td>hr</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...</td>\n",
       "      <td>az</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>Nella serie \"Magico Vento\" figura il personagg...</td>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...</td>\n",
       "      <td>zh</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang language\n",
       "id                                                                     \n",
       "1       Der Flughafen Berlin Brandenburg verfügt über ...   de         \n",
       "2       Успешное развитие общества, однако, возможно л...   ru         \n",
       "3       I øvrigt er kendetegnene for en magnetisk svag...   da         \n",
       "4       Sowohl über den historischen Simon als auch üb...   de         \n",
       "5       Emmure е формирана през 2003 г., когато Франки...   bg         \n",
       "...                                                   ...  ...      ...\n",
       "399994  Sociální náklady stejně jako soukromé náklady ...   cs         \n",
       "399995  Sljedećeg dana, glumac je malo zakasnio na set...   hr         \n",
       "399996  İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...   az         \n",
       "399998  Nella serie \"Magico Vento\" figura il personagg...   it         \n",
       "399999  經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...   zh         \n",
       "\n",
       "[320000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text lang language\n",
      "id                                                                     \n",
      "1       Der Flughafen Berlin Brandenburg verfügt über ...   de         \n",
      "2       Успешное развитие общества, однако, возможно л...   ru         \n",
      "3       I øvrigt er kendetegnene for en magnetisk svag...   da         \n",
      "4       Sowohl über den historischen Simon als auch üb...   de         \n",
      "5       Emmure е формирана през 2003 г., когато Франки...   bg         \n",
      "...                                                   ...  ...      ...\n",
      "399994  Sociální náklady stejně jako soukromé náklady ...   cs         \n",
      "399995  Sljedećeg dana, glumac je malo zakasnio na set...   hr         \n",
      "399996  İstiqlal Sarayı (), Yenidən Birleşme Sarayı ()...   az         \n",
      "399998  Nella serie \"Magico Vento\" figura il personagg...   it         \n",
      "399999  經過多年努力,物理學者仍舊無法將引力併入量子框架,更不必說與其他基本力統合。因此,他們轉移工...   zh         \n",
      "\n",
      "[320000 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_680/389421962.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[pred_latin][\"language\"] = \"de\" # predictions\n"
     ]
    }
   ],
   "source": [
    "# Classify all latin-languages\n",
    "# text_val_latin_only = df[pred_latin][\"text\"]\n",
    "# \n",
    "# remove_punctuation = str.maketrans('', '', r\"-()\\\"#/@;:<>{}-=~|.?,\")\n",
    "# \n",
    "# def PunctFreeLower(texts):\n",
    "#     cleaned = []\n",
    "#     for text in texts:\n",
    "#         cleaned.append((re.sub(r\"[0-9]+\", \"\", (text.translate(remove_punctuation)))).lower())\n",
    "#     return cleaned\n",
    "# \n",
    "# text_val_cleaned = PunctFreeLower(text_val_latin_only[\"text\"])\n",
    "# \n",
    "# vec2 = CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "# text_val_vec2 = vec2.fit_transform(text_val_cleaned)\n",
    "#    \n",
    "# # Load the model and make predictions\n",
    "# model = open(Path().resolve() / \"model.joblib\")\n",
    "# predictions = model.predict(text_val_vec2)\n",
    "df[pred_latin][\"language\"] = \"de\" # predictions\n",
    "print(df)\n",
    "df_ = df[\"language\"]\n",
    "\n",
    "# Save the predictions\n",
    "df_.to_json(\n",
    "    Path().resolve() / \"predictions.jsonl\", orient=\"index\", index=True\n",
    ")\n",
    "# df_.to_json(\n",
    "#     Path().resolve() / \"predictions.jsonl\", orient=\"records\", lines=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
